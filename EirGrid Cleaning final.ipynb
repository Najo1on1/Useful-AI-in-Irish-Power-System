{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install pytorch-lightning --quiet\n",
        "#!pip install transformers --quiet\n",
        "#!pip install accelerate --quiet\n",
        "#!pip install pytorch-forecasting --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core data & ML libs\n",
        "# !pip install numpy --quiet\n",
        "# !pip install pandas --quiet\n",
        "# !pip install scikit-learn --quiet\n",
        "# !pip install torch --quiet\n",
        "# !pip install matplotlib --quiet\n",
        "# !pip install seaborn --quiet\n",
        "# !pip install scipy --quiet\n",
        "# !pip install boto3 --quiet\n",
        "# !pip install requests --quiet\n",
        "\n",
        "# Time-series / forecasting\n",
        "# !pip install pytorch-lightning --quiet\n",
        "# !pip install pytorch-forecasting --quiet\n",
        "\n",
        "# (Optional, if you need them)\n",
        "# !pip install transformers --quiet\n",
        "# !pip install accelerate --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === minimal imports actually used in this notebook ===\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import deque\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import make_scorer, mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === download EirGrid XLSX reports from the provided links ===\n",
        "DOWNLOAD_DIR = r\"D:\\Colab\\Summer Project\\Eirgrid\\Reports\\downloaded\"\n",
        "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
        "\n",
        "urls = {\n",
        "    \"System-Data-Qtr-Hourly-2014-2015.xlsx\": \"https://www.eirgrid.ie/site-files/library/EirGrid/System-Data-Qtr-Hourly-2014-2015.xlsx\",\n",
        "    \"System-Data-Qtr-Hourly-2016-2017.xlsx\": \"https://www.eirgrid.ie/site-files/library/EirGrid/System-Data-Qtr-Hourly-2016-2017.xlsx\",\n",
        "    \"System-Data-Qtr-Hourly-2018-2019.xlsx\": \"https://www.eirgrid.ie/site-files/library/EirGrid/System-Data-Qtr-Hourly-2018-2019.xlsx\",\n",
        "    \"System-Data-Qtr-Hourly-2020-2021.xlsx\": \"https://www.eirgrid.ie/site-files/library/EirGrid/System-Data-Qtr-Hourly-2020-2021.xlsx\",\n",
        "    \"System-Data-Qtr-Hourly-2022-2023_0.xlsx\": \"https://www.eirgrid.ie/site-files/library/EirGrid/System-Data-Qtr-Hourly-2022-2023_0.xlsx\",\n",
        "    \"System-Data-Qtr-Hourly-2024.xlsx\": \"https://www.eirgrid.ie/site-files/library/EirGrid/System-Data-Qtr-Hourly-2024.xlsx\",\n",
        "    \"System-Data-Qtr-Hourly-2025.xlsx\": \"https://www.eirgrid.ie/site-files/library/EirGrid/System-Data-Qtr-Hourly-2025.xlsx\",\n",
        "}\n",
        "\n",
        "downloaded_paths = {}\n",
        "for fname, url in urls.items():\n",
        "    out_path = os.path.join(DOWNLOAD_DIR, fname)\n",
        "    if not os.path.exists(out_path):\n",
        "        r = requests.get(url, timeout=120)\n",
        "        r.raise_for_status()\n",
        "        with open(out_path, \"wb\") as f:\n",
        "            f.write(r.content)\n",
        "    downloaded_paths[fname] = out_path\n",
        "\n",
        "# convenience aliases for the rest of the notebook (match prior variable names)\n",
        "file_2014_2015 = downloaded_paths[\"System-Data-Qtr-Hourly-2014-2015.xlsx\"]\n",
        "file_2016_2017 = downloaded_paths[\"System-Data-Qtr-Hourly-2016-2017.xlsx\"]\n",
        "file_2018_2019 = downloaded_paths[\"System-Data-Qtr-Hourly-2018-2019.xlsx\"]\n",
        "file_2020_2021 = downloaded_paths[\"System-Data-Qtr-Hourly-2020-2021.xlsx\"]\n",
        "file_2022_2023 = downloaded_paths[\"System-Data-Qtr-Hourly-2022-2023_0.xlsx\"]\n",
        "file_2024      = downloaded_paths[\"System-Data-Qtr-Hourly-2024.xlsx\"]\n",
        "file_2025      = downloaded_paths[\"System-Data-Qtr-Hourly-2025.xlsx\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===download ONLY interconnection data (2014 → today) ===\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "def fetch_interconnection(region: str = \"ALL\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Download monthly JSON slices from Smart Grid Dashboard and return a single\n",
        "    DataFrame with columns: EffectiveTime (datetime), Value (float), Region (str).\n",
        "    \"\"\"\n",
        "    # month starts from 2014-01 to the first day of next month (inclusive bound for loop)\n",
        "    months = pd.period_range(start=\"2014-01\", end=pd.Timestamp.today().strftime(\"%Y-%m\"), freq=\"M\")\n",
        "    frames = []\n",
        "\n",
        "    for per in months:\n",
        "        start = per.start_time.replace(day=1, hour=0, minute=0, second=0, microsecond=0)\n",
        "        # next month \"01-<mon>-YY 21:59\" per reference implementation\n",
        "        end   = (per.asfreq(\"M\").to_timestamp() + pd.offsets.MonthBegin(1)).replace(hour=21, minute=59)\n",
        "\n",
        "        # format like 01-Jan-14 HH:MM\n",
        "        df_str = start.strftime(\"%d-%b-%y %H:%M\")\n",
        "        dt_str = end.strftime(\"%d-%b-%y %H:%M\")\n",
        "\n",
        "        url = (\n",
        "            \"https://www.smartgriddashboard.com/DashboardService.svc/data\"\n",
        "            f\"?area=interconnection&region={region}\"\n",
        "            f\"&datefrom={df_str.replace(':','%3A')}\"\n",
        "            f\"&dateto={dt_str.replace(':','%3A')}\"\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            r = requests.get(url, timeout=30)\n",
        "            r.raise_for_status()\n",
        "            rows = r.json().get(\"Rows\", [])\n",
        "            if not rows:\n",
        "                continue\n",
        "            frames.append(pd.DataFrame(rows))\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ {region} {per.strftime('%Y-%m')}: {e}\")\n",
        "\n",
        "    if not frames:\n",
        "        raise RuntimeError(\"No interconnection data retrieved.\")\n",
        "\n",
        "    df = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "    # Normalize expected columns\n",
        "    # Keep only EffectiveTime + Value (+ Region if present)\n",
        "    col_time = \"EffectiveTime\" if \"EffectiveTime\" in df.columns else \"DateTime\"\n",
        "    col_val  = \"Value\" if \"Value\" in df.columns else \"value\"\n",
        "\n",
        "    df = df[[c for c in [col_time, col_val, \"Region\"] if c in df.columns]].copy()\n",
        "    df.rename(columns={col_time: \"EffectiveTime\", col_val: \"Value\"}, inplace=True)\n",
        "    df[\"EffectiveTime\"] = pd.to_datetime(df[\"EffectiveTime\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"EffectiveTime\"]).sort_values(\"EffectiveTime\").drop_duplicates(\"EffectiveTime\")\n",
        "\n",
        "    # If Region not supplied by API, stamp it\n",
        "    if \"Region\" not in df.columns:\n",
        "        df[\"Region\"] = region\n",
        "\n",
        "    return df\n",
        "\n",
        "# Download ALL-region interconnection once; keep in memory\n",
        "df_intercon_all = fetch_interconnection(region=\"ALL\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Processing using the downloaded interconnection ===\n",
        "# Columns needed across years\n",
        "cols_df3 = [\n",
        "    'DateTime',\n",
        "    'NI Generation','NI Demand','NI Wind Availability','NI Wind Generation',\n",
        "    'IE Generation','IE Demand','IE Wind Availability','IE Wind Generation',\n",
        "    'SNSP'\n",
        "]\n",
        "\n",
        "def read_sysdata(path):\n",
        "    return pd.read_excel(path, sheet_name=\"System Data\", parse_dates=[\"DateTime\"])\n",
        "\n",
        "# Load per-period (files defined in part A after download)\n",
        "df_2014_2015 = read_sysdata(file_2014_2015)[cols_df3]\n",
        "df_2016_2017 = read_sysdata(file_2016_2017)[cols_df3]\n",
        "df_2018_2019 = read_sysdata(file_2018_2019)[cols_df3]\n",
        "df_2020_2021 = read_sysdata(file_2020_2021)[cols_df3]\n",
        "df_2022_2023 = read_sysdata(file_2022_2023)[cols_df3]\n",
        "df_2024      = read_sysdata(file_2024)[cols_df3]\n",
        "df_2025      = read_sysdata(file_2025)[cols_df3]\n",
        "\n",
        "# Stack all years\n",
        "df_eir3 = pd.concat([\n",
        "    df_2014_2015, df_2016_2017, df_2018_2019,\n",
        "    df_2020_2021, df_2022_2023, df_2024, df_2025\n",
        "], ignore_index=True)\n",
        "\n",
        "# Enforce datetime and sort once\n",
        "df_eir3['DateTime'] = pd.to_datetime(df_eir3['DateTime'])\n",
        "df_eir3 = df_eir3.sort_values('DateTime').reset_index(drop=True)\n",
        "\n",
        "# Use the freshly downloaded interconnection: take Value as the series\n",
        "df_intercon = (\n",
        "    df_intercon_all[['EffectiveTime', 'Value']]\n",
        "    .rename(columns={'EffectiveTime': 'DateTime', 'Value': 'interconnection'})\n",
        ")\n",
        "df_intercon['DateTime'] = pd.to_datetime(df_intercon['DateTime'])\n",
        "\n",
        "# Merge (left) without filling interconnection yet\n",
        "df_eir3 = df_eir3.merge(df_intercon, on='DateTime', how='left')\n",
        "\n",
        "# Quick check: sorted ascending\n",
        "assert df_eir3['DateTime'].is_monotonic_increasing, \"DateTime not sorted ascending.\"\n",
        "print(\"Loaded & merged with downloaded interconnection. Rows:\", len(df_eir3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fill non-target features to remove accidental gaps; keep interconnection untouched\n",
        "_inter_orig = df_eir3['interconnection'].copy()\n",
        "to_fill = [c for c in df_eir3.columns if c != 'interconnection']\n",
        "df_eir3[to_fill] = df_eir3[to_fill].bfill().ffill()\n",
        "df_eir3['interconnection'] = _inter_orig\n",
        "\n",
        "# Set index for rolling ops\n",
        "df_eir3 = df_eir3.set_index('DateTime')\n",
        "\n",
        "# Helper: rolling means on 15-min data\n",
        "def roll_mean(series, hours):\n",
        "    window = int(hours * 4)  # 4 samples/hour\n",
        "    return series.rolling(window=window, min_periods=window).mean()\n",
        "\n",
        "# --- Non-interconnection features used in raw_top20 ---\n",
        "# Net loads\n",
        "df_eir3['NetLoad_IE'] = df_eir3['IE Demand'] - df_eir3['IE Wind Generation']\n",
        "df_eir3['NetLoad_NI'] = df_eir3['NI Demand'] - df_eir3['NI Wind Generation']\n",
        "\n",
        "# Rolling means required (1h = 4, 2h = 8, 4h = 16)\n",
        "df_eir3['IE_WindGen_rollmean_1h'] = roll_mean(df_eir3['IE Wind Generation'], 1)\n",
        "df_eir3['IE_WindGen_rollmean_2h'] = roll_mean(df_eir3['IE Wind Generation'], 2)\n",
        "df_eir3['IE_WindGen_rollmean_4h'] = roll_mean(df_eir3['IE Wind Generation'], 4)\n",
        "\n",
        "df_eir3['IE_Demand_rollmean_1h']  = roll_mean(df_eir3['IE Demand'], 1)\n",
        "df_eir3['IE_Demand_rollmean_4h']  = roll_mean(df_eir3['IE Demand'], 4)\n",
        "\n",
        "df_eir3['NI_Demand_rollmean_1h']  = roll_mean(df_eir3['NI Demand'], 1)\n",
        "df_eir3['NI_Demand_rollmean_2h']  = roll_mean(df_eir3['NI Demand'], 2)\n",
        "df_eir3['NI_Demand_rollmean_4h']  = roll_mean(df_eir3['NI Demand'], 4)\n",
        "\n",
        "df_eir3['NI_WindGen_rollmean_1h'] = roll_mean(df_eir3['NI Wind Generation'], 1)\n",
        "\n",
        "# --- Interconnection-based features (for training rows where interconnection exists) ---\n",
        "# Lags\n",
        "df_eir3['intercon_lag_1'] = df_eir3['interconnection'].shift(1)\n",
        "df_eir3['intercon_lag_4'] = df_eir3['interconnection'].shift(4)\n",
        "\n",
        "# 4h rolling mean (16 samples)\n",
        "df_eir3['intercon_rollmean_4h'] = df_eir3['interconnection'].rolling(window=16, min_periods=16).mean()\n",
        "\n",
        "# 4h slope via simple OLS slope on last 16 points\n",
        "def rolling_slope(series, window=16):\n",
        "    # compute slope for each rolling window using polyfit on index 0..window-1\n",
        "    idx = np.arange(window)\n",
        "    def _s(x):\n",
        "        if np.isnan(x).any():\n",
        "            return np.nan\n",
        "        # slope only\n",
        "        return np.polyfit(idx, x, 1)[0]\n",
        "    return series.rolling(window=window, min_periods=window).apply(_s, raw=True)\n",
        "\n",
        "df_eir3['intercon_slope_4h'] = rolling_slope(df_eir3['interconnection'], window=16)\n",
        "\n",
        "# Restore DateTime as column\n",
        "df_eir3 = df_eir3.reset_index()\n",
        "\n",
        "# Minimal NaN report (mostly first ~16 rows due to roll windows or missing interconnection)\n",
        "nan_counts = df_eir3.isna().sum()\n",
        "print(\"NaNs present in:\\n\", nan_counts[nan_counts>0].sort_values(ascending=False).head(15))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The fixed top-20 features you derived\n",
        "raw_top20 = [\n",
        "    'intercon_lag_1',\n",
        "    'IE_WindGen_rollmean_1h',\n",
        "    'NetLoad_IE',\n",
        "    'intercon_rollmean_4h',\n",
        "    'IE Wind Generation',\n",
        "    'IE_Demand_rollmean_4h',\n",
        "    'NI_Demand_rollmean_2h',\n",
        "    'IE Demand',\n",
        "    'intercon_lag_4',\n",
        "    'IE_Demand_rollmean_1h',\n",
        "    'IE_WindGen_rollmean_4h',\n",
        "    'NetLoad_NI',\n",
        "    'NI Demand',\n",
        "    'intercon_slope_4h',\n",
        "    'NI_Demand_rollmean_1h',\n",
        "    'IE Generation',\n",
        "    'NI_WindGen_rollmean_1h',\n",
        "    'NI_Demand_rollmean_4h',\n",
        "    'IE_WindGen_rollmean_2h',\n",
        "    'NI Wind Generation'\n",
        "]\n",
        "\n",
        "# Build training set: rows with real interconnection and all needed features available (after 16-sample warmup)\n",
        "feat_present_mask = df_eir3[raw_top20].notna().all(axis=1)\n",
        "train_mask = df_eir3['interconnection'].notna() & feat_present_mask\n",
        "\n",
        "df_train = df_eir3.loc[train_mask].copy()\n",
        "\n",
        "X_train = df_train[raw_top20]\n",
        "y_train = df_train['interconnection']\n",
        "\n",
        "# Scale + grid search (time-series CV)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = pd.DataFrame(\n",
        "    scaler.fit_transform(X_train),\n",
        "    columns=raw_top20,\n",
        "    index=X_train.index\n",
        ")\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "rmse_scorer = make_scorer(\n",
        "    lambda y_true, y_pred: -np.sqrt(mean_squared_error(y_true, y_pred)),\n",
        "    greater_is_better=True\n",
        ")\n",
        "\n",
        "ridge_param_grid = {\n",
        "    'alpha':    [1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100],\n",
        "    'solver':   ['saga', 'sag'],\n",
        "    'tol':      [1e-2, 1e-3, 1e-4],\n",
        "    'max_iter': [500, 1000, 5000]\n",
        "}\n",
        "\n",
        "ridge_grid = GridSearchCV(\n",
        "    estimator=Ridge(random_state=0),\n",
        "    param_grid=ridge_param_grid,\n",
        "    cv=tscv,\n",
        "    scoring=rmse_scorer,\n",
        "    n_jobs=-1,\n",
        "    return_train_score=True\n",
        ")\n",
        "ridge_grid.fit(X_scaled, y_train)\n",
        "\n",
        "best_params = ridge_grid.best_params_\n",
        "final_ridge = Ridge(\n",
        "    alpha=best_params['alpha'],\n",
        "    solver=best_params['solver'],\n",
        "    tol=best_params['tol'],\n",
        "    max_iter=best_params['max_iter'],\n",
        "    random_state=0\n",
        ")\n",
        "final_ridge.fit(X_scaled, y_train)\n",
        "\n",
        "print(\"Best Ridge params:\", best_params)\n",
        "print(\"TS CV RMSE (neg scorer):\", -ridge_grid.best_score_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# quick holdout using 2022-01-01 forward (requires those dates exist with real interconnection)\n",
        "cutoff_val = pd.Timestamp(\"2022-01-01\")\n",
        "mask_val = (df_train['DateTime'] >= cutoff_val)\n",
        "X_val = df_train.loc[mask_val, raw_top20]\n",
        "y_val = df_train.loc[mask_val, 'interconnection']\n",
        "\n",
        "X_val_scaled = pd.DataFrame(scaler.transform(X_val), columns=raw_top20, index=X_val.index)\n",
        "y_pred_val = final_ridge.predict(X_val_scaled)\n",
        "rmse_val = np.sqrt(mean_squared_error(y_val, y_pred_val))\n",
        "print(f\"Hold-out RMSE (>=2022-01-01): {rmse_val:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Impute strictly after the date you chose; adjust if needed to match your earlier cut\n",
        "start_impute = pd.Timestamp(\"2024-02-20 00:00:00\")  # first timestamp to impute (exclusive of 2024-02-19)\n",
        "end_impute   = pd.Timestamp(\"2025-04-30 23:45:00\")  # inclusive end bound\n",
        "\n",
        "# Build index to impute (ensure it exists in df_eir3)\n",
        "mask_imp = (df_eir3['DateTime'] >= start_impute) & (df_eir3['DateTime'] <= end_impute)\n",
        "imp_index = df_eir3.index[mask_imp]\n",
        "\n",
        "if len(imp_index) == 0:\n",
        "    print(\"No rows to impute in the requested range.\")\n",
        "else:\n",
        "    # Initialize a 16-sample buffer with the last known interconnection values BEFORE start_impute\n",
        "    pre_window_end = df_eir3.index[df_eir3['DateTime'] < start_impute]\n",
        "    if len(pre_window_end) == 0:\n",
        "        raise ValueError(\"No history before start_impute to seed the interconnection buffer.\")\n",
        "\n",
        "    # Gather last 16 known/pred interconnection values (must be non-NaN)\n",
        "    # If your history still has NaNs close to the boundary, you may need to walk back further.\n",
        "    hist_mask = (df_eir3['DateTime'] < start_impute) & (df_eir3['interconnection'].notna())\n",
        "    last16 = df_eir3.loc[hist_mask, 'interconnection'].tail(16).values\n",
        "\n",
        "    if len(last16) < 16:\n",
        "        raise ValueError(\"Need at least 16 historical interconnection values to start imputation.\")\n",
        "\n",
        "    buffer = deque(last16.tolist(), maxlen=16)\n",
        "\n",
        "    # Precompute scaled non-interconnection features for the whole dataset so we can grab them quickly\n",
        "    # We'll reuse the same StandardScaler fitted on training, so at predict-time we build a row in raw_top20 order.\n",
        "    # Non-interconnection predictors used in raw_top20 (already computed in Cell 2) are:\n",
        "    # IE_WindGen_rollmean_1h/2h/4h, IE_Demand_rollmean_1h/4h, NI_Demand_rollmean_1h/2h/4h, NI_WindGen_rollmean_1h,\n",
        "    # NetLoad_IE, NetLoad_NI, IE/NI Demand, IE/NI Wind Generation, IE Generation.\n",
        "\n",
        "    for idx in imp_index:\n",
        "        row = df_eir3.loc[idx]\n",
        "\n",
        "        # Build the 4 interconnection features from the buffer\n",
        "        intercon_lag_1 = buffer[-1]\n",
        "        intercon_lag_4 = buffer[-4] if len(buffer) >= 4 else np.nan\n",
        "        intercon_rollmean_4h = float(np.mean(buffer))\n",
        "        # slope over last 16 points\n",
        "        x = np.arange(len(buffer))\n",
        "        slope = float(np.polyfit(x, np.array(buffer, dtype=float), 1)[0])\n",
        "\n",
        "        # Collect the rest of the features directly from precomputed columns at this timestamp\n",
        "        feat_values = {\n",
        "            'intercon_lag_1': intercon_lag_1,\n",
        "            'IE_WindGen_rollmean_1h': row['IE_WindGen_rollmean_1h'],\n",
        "            'NetLoad_IE': row['NetLoad_IE'],\n",
        "            'intercon_rollmean_4h': intercon_rollmean_4h,\n",
        "            'IE Wind Generation': row['IE Wind Generation'],\n",
        "            'IE_Demand_rollmean_4h': row['IE_Demand_rollmean_4h'],\n",
        "            'NI_Demand_rollmean_2h': row['NI_Demand_rollmean_2h'],\n",
        "            'IE Demand': row['IE Demand'],\n",
        "            'intercon_lag_4': intercon_lag_4,\n",
        "            'IE_Demand_rollmean_1h': row['IE_Demand_rollmean_1h'],\n",
        "            'IE_WindGen_rollmean_4h': row['IE_WindGen_rollmean_4h'],\n",
        "            'NetLoad_NI': row['NetLoad_NI'],\n",
        "            'NI Demand': row['NI Demand'],\n",
        "            'intercon_slope_4h': slope,\n",
        "            'NI_Demand_rollmean_1h': row['NI_Demand_rollmean_1h'],\n",
        "            'IE Generation': row['IE Generation'],\n",
        "            'NI_WindGen_rollmean_1h': row['NI_WindGen_rollmean_1h'],\n",
        "            'NI_Demand_rollmean_4h': row['NI_Demand_rollmean_4h'],\n",
        "            'IE_WindGen_rollmean_2h': row['IE_WindGen_rollmean_2h'],\n",
        "            'NI Wind Generation': row['NI Wind Generation'],\n",
        "        }\n",
        "\n",
        "        # Make a 1-row DataFrame in the correct column order\n",
        "        X_one = pd.DataFrame([[feat_values[k] for k in raw_top20]], columns=raw_top20, index=[idx])\n",
        "\n",
        "        # Scale and predict\n",
        "        X_one_scaled = pd.DataFrame(scaler.transform(X_one), columns=raw_top20, index=[idx])\n",
        "        y_hat = float(final_ridge.predict(X_one_scaled)[0])\n",
        "\n",
        "        # Write back prediction and update buffer\n",
        "        df_eir3.at[idx, 'interconnection'] = y_hat\n",
        "        buffer.append(y_hat)\n",
        "\n",
        "    print(f\"Imputed interconnection for {len(imp_index)} timestamps from {start_impute} to {end_impute}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Keep only the 7 required columns\n",
        "df_eir = df_eir3[['DateTime',\n",
        "                  'IE Generation','IE Demand',\n",
        "                  'IE Wind Availability','IE Wind Generation',\n",
        "                  'SNSP','interconnection']].copy()\n",
        "\n",
        "# Enforce time bounds\n",
        "start_bound = pd.Timestamp(\"2014-01-01 00:00:00\")\n",
        "end_bound   = pd.Timestamp(\"2025-04-30 23:45:00\")\n",
        "df_eir = df_eir[(df_eir['DateTime'] >= start_bound) & (df_eir['DateTime'] <= end_bound)].copy()\n",
        "\n",
        "# Sort and basic integrity checks\n",
        "df_eir = df_eir.sort_values('DateTime').reset_index(drop=True)\n",
        "\n",
        "# 15-min regularity assertion (no reindexing – just check)\n",
        "dt = df_eir['DateTime'].diff().dropna().unique()\n",
        "ok_15m = (len(dt) == 1) and (pd.Timedelta(minutes=15) == dt[0])\n",
        "print(\"15-min regularity:\", ok_15m)\n",
        "\n",
        "# No missing values in the final 7 columns\n",
        "missing_final = df_eir.isna().sum()\n",
        "print(\"Final NaNs by column:\\n\", missing_final)\n",
        "\n",
        "print(\"Final df_eir shape:\", df_eir.shape)\n",
        "df_eir.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "najo1 (3.12.3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
